# TP3 SIA - Perceptr√≥n Simple y Multicapa

## üëã Introducci√≥n

Trabajo pr√°ctico para la materia de Sistemas de Inteligencia Artificial en el ITBA. Se busc√≥ implementar un perceptr√≥n escal√≥n, perceptr√≥n lineal, perceptr√≥n no lineal y perceptr√≥n multicapa gen√©rico que, al hacer un main que genere un dataframe con las columnas "x1", "x2", ... "xn", "ev" y se lo pases a estos, se pueda categorizar resultados utilizando el modelo de la neurona. En los mains de este proyecto se agregan algunos problemas de ejemplo (tomados del enunciado de m√°s abajo) y la posibilidad de alterar los hiperpar√°metros en la configuraci√≥n previa a correr los mains.

Se cuenta con 3 mains:
- `main_step_perceptron.py` (perceptr√≥n escal√≥n)
- `main_uniform_perceptron.py` (integra perceptr√≥n lineal y perceptr√≥n no lineal en uno)
- `main_multilayer_perceptron.py`(perceptr√≥n multicapa)

Este fue el [Enunciado](docs/Enunciado%20TP3.pdf)

### ‚ùó Requisitos

- Python3 (La aplicaci√≥n se prob√≥ en la versi√≥n de Python 3.11.*)
- pip3
- [pipenv](https://pypi.org/project/pipenv)

### üíª Instalaci√≥n

En caso de no tener python, descargarlo desde la [p√°gina oficial](https://www.python.org/downloads/release/python-3119/)

Utilizando pip (o pip3 en mac/linux) instalar la dependencia de **pipenv**:

```sh
pip install pipenv
```

Parado en la carpeta del proyecto ejecutar:

```sh
pipenv install
```

para instalar las dependencias necesarias en el ambiente virtual.

## üõ†Ô∏è Configuraci√≥n
En este proyecto se cuenta con 3 archivos de configuraci√≥n en la carpeta `configs`, cada uno para su main correspondiente. 

Si quer√©s configurar el main `main_step_perceptron.py`, ten√©s el archivo [`config_step.json`](configs/config_step.json) que cuenta con los hiperpar√°metros:
- `learn_rate`: [float] la tasa de aprendizaje para la actualizaci√≥n de pesos
- `max_epochs`: [int] la cantidad de √©pocas m√°xima antes de dejar de iterar
- `random_weight_initialize`: [bool] True para empezar con pesos aleatorios entre -1 y 1, False para empezar con pesos en 0
- `test_and`: [bool] True para probar el problema And, False para el problema XOR

Si quer√©s configurar el main `main_uniform_perceptron.py`, ten√©s el archivo [`config_uniform.json`](configs/config_uniform.json) que cuenta con los hiperpar√°metros:
- `test_percentage`: [float entre 0-1] para elegir el porcentaje tomado del dataset para ser testing set
- `test_with_train_data`: [bool] True para no separar en train dataset y test dataset, y simplemente entrenar y luego testear con el mismo dataset completo (√∫til para asegurarse que funciona el entrenamiento), false en el caso contrario
- `learn_rate`: [float] la tasa de aprendizaje para la actualizaci√≥n de pesos
- `max_epochs`: [int] la cantidad de √©pocas m√°xima antes de dejar de iterar
- `random_weight_initialize`: [bool] True para empezar con pesos aleatorios entre -1 y 1, False para empezar con pesos en 0
- `function`: [str entre "LINEAR", "LOGISTICS", "HYPERBOLIC"] la funci√≥n de activaci√≥n a usar, los expected values se normalizar√°n para este main acorde a sus im√°genes
- `beta`: [float] el beta en caso de usar la funci√≥n LOGISTICS/HYPERBOLIC, se recomienda usar 1.0
- `min_error`: [float] el error m√≠nimo para dejar de iterar
- `batch_update`: [int] para este main solo se agrega la posibilidad de modificar los pesos cada cierto n√∫mero de datos, el valor ingresado ser√° ese n√∫mero

Si quer√©s configurar el main `main_multilayer_perceptron.py`, ten√©s el archivo [`config_multilayer.json`](configs/config_multilayer.json) que cuenta con los hiperpar√°metros:
- `problem`: [str entre "XOR", "PARITY", "DIGITS"] para elegir el problema a resolver
- `hidden_layers`: [List[int]] arreglo con la cantidad de neuronas para cada capa oculta, la cantidad de elementos en el arreglo ser√° la cantidad de capas ocultas
- `test_percentage`: [float entre 0-1] para elegir el porcentaje tomado del dataset para ser testing set
- `test_with_train_data`: [bool] True para no separar en train dataset y test dataset, y simplemente entrenar y luego testear con el mismo dataset completo (√∫til para asegurarse que funciona el entrenamiento). false en el caso contrario
- `learn_rate`: [float] la tasa de aprendizaje para la actualizaci√≥n de pesos
- `max_epochs`: [int] la cantidad de √©pocas m√°xima antes de dejar de iterar
- `random_weight_initialize`: [bool] True para empezar con pesos aleatorios entre -1 y 1, False para empezar con pesos en 0
- `function`: [str entre "LINEAR", "LOGISTICS", "HYPERBOLIC"] la funci√≥n de activaci√≥n a usar
- `beta`: [float] el beta en caso de usar la funci√≥n LOGISTICS/HYPERBOLIC, se recomienda usar 1.0
- `min_error`: [float] el error m√≠nimo para dejar de iterar
- `optimizer`: [str entre "GRADIENT_DESCENT", "MOMENTUM" y "ADAM"] para elegir el optimizar a utilizar con el que se va a actualizar los pesos, esto est√° solo para este main

## üèÉ Ejecuci√≥n

Para probar la aplicaci√≥n, correr:
```shell
pipenv run python <main deseado>
```
Siendo \<main deseado> una de estas opciones:
- `main_step_perceptron.py`
- `main_uniform_perceptron.py` 
- `main_multilayer_perceptron.py`

Se imprimir√° en stdout el error y √©poca actual. En el caso del `main_step_perceptron.py`, se generar√° un GIF para ver como se separa linealmente los datos (por esta generaci√≥n de GIF, este perceptr√≥n va un poco m√°s lento que los otros).

Para abrir el google Colab d√≥nde se realizaron las pruebas ir al siguiente [link](https://colab.research.google.com/drive/1g6m9iIVd1q4IE0GQ9GoGSwTa-zh8L5bb?usp=sharing)
